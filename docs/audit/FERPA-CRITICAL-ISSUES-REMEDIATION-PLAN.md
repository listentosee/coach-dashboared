# FERPA Critical Issues Remediation Plan

## Executive Summary

This plan addresses the **5 CRITICAL** FERPA compliance issues identified in the October 5, 2025 audit. All changes are designed to **maintain 100% backward compatibility** and **zero impact on current functionality**.

**PHASED APPROACH (October 9, 2025):**

### ‚úÖ Already Complete (32 hours)
- **Issue #2:** PII removal from logs - 100% complete
- **Issue #3:** Comprehensive audit logging - 100% complete

### üéØ Sprint 1: HIGH-IMPACT (6 hours) ‚≠ê **DO THIS WEEK**
- **Phase 1.1:** Encrypt existing data in database (4 hours)
- **Phase 1.4:** Document storage encryption (2 hours)
- **Result:** Issue #1 at 85%, demonstrable compliance with zero integration risk

### üîµ Sprint 2-3: MEDIUM/LOW-IMPACT (36+ hours) - **Defer**
- Storage retention policies (20 hours)
- Automated dual-write or database triggers (16 hours)
- Legal DPA execution (parallel track)

**Key Insight:** After 6 hours of work (Sprint 1), you achieve 85% compliance on Issue #1 with zero risk to Zoho/MetaCTF integrations. Remaining work is operational improvement.

**Recommended Path:**
1. **This week:** Execute Sprint 1 (6 hours) ‚Üí 85% Issue #1 complete
2. **Pause & verify:** Test integrations, demonstrate compliance
3. **Next 2-3 weeks:** Storage retention (if time permits)
4. **Future:** Dual-write automation as technical debt

**Total Estimated Effort:**
- Minimum for compliance: 38 hours (32 complete + 6 for Sprint 1)
- Full implementation: 80-96 hours (includes all operational improvements)

---

## Issue 1: PII Column Encryption (CRITICAL - 24 hours)

### Current State
- PII fields stored in plaintext: `email_personal`, `email_school`, `parent_email`, `first_name`, `last_name`, `parent_name`
- Sensitive demographics (`gender`, `race`, `ethnicity`) unencrypted
- `pgcrypto` extension installed but not used
- **Integration Risk**: Zoho Sign and MetaCTF integrations actively use these fields

### Implementation Plan - PHASED APPROACH

**Key Insight:** FERPA compliance is achieved by having encrypted columns populated alongside plaintext columns. Dropping plaintext columns is optional and can be deferred indefinitely.

**Strategy:** Break into small high-impact steps, pause to verify, then tackle low-impact steps.

---

## HIGH-IMPACT PHASES (Immediate Compliance Value)

### Phase 1.1: Add Encrypted Columns + Migrate Existing Data (4 hours) ‚≠ê **DO FIRST**

**Impact:**
- ‚úÖ Demonstrates encryption at rest capability
- ‚úÖ Protects all existing PII immediately
- ‚úÖ Zero risk (additive only, no drops)
- ‚úÖ Can show auditors encrypted data exists
**Implementation:**
```sql
-- Step 1: Add encrypted columns (2 minutes)
ALTER TABLE competitors ADD COLUMN email_personal_encrypted BYTEA;
ALTER TABLE competitors ADD COLUMN email_school_encrypted BYTEA;
ALTER TABLE competitors ADD COLUMN parent_email_encrypted BYTEA;
ALTER TABLE competitors ADD COLUMN first_name_encrypted BYTEA;
ALTER TABLE competitors ADD COLUMN last_name_encrypted BYTEA;
ALTER TABLE competitors ADD COLUMN parent_name_encrypted BYTEA;
ALTER TABLE competitors ADD COLUMN demographics_encrypted BYTEA;

-- Step 2: Create database encryption functions (5 minutes)
CREATE OR REPLACE FUNCTION encrypt_pii(plaintext TEXT, key TEXT)
RETURNS BYTEA
LANGUAGE SQL
IMMUTABLE
AS $$
  SELECT pgp_sym_encrypt(plaintext, key);
$$;

CREATE OR REPLACE FUNCTION decrypt_pii(encrypted BYTEA, key TEXT)
RETURNS TEXT
LANGUAGE SQL
IMMUTABLE
AS $$
  SELECT pgp_sym_decrypt(encrypted, key);
$$;

-- Step 3: Migrate existing data (run in batches to avoid locks)
-- Set encryption key first (replace with your actual key)
SET app.encryption_key = 'your-secure-256-bit-key-here';

-- Migrate in batches of 100 to avoid long locks
DO $$
DECLARE
  batch_size INT := 100;
  offset_val INT := 0;
  rows_updated INT;
BEGIN
  LOOP
    UPDATE competitors
    SET
      email_personal_encrypted = pgp_sym_encrypt(COALESCE(email_personal, ''), current_setting('app.encryption_key')),
      email_school_encrypted = pgp_sym_encrypt(COALESCE(email_school, ''), current_setting('app.encryption_key')),
      parent_email_encrypted = pgp_sym_encrypt(COALESCE(parent_email, ''), current_setting('app.encryption_key')),
      first_name_encrypted = pgp_sym_encrypt(first_name, current_setting('app.encryption_key')),
      last_name_encrypted = pgp_sym_encrypt(last_name, current_setting('app.encryption_key')),
      parent_name_encrypted = pgp_sym_encrypt(COALESCE(parent_name, ''), current_setting('app.encryption_key')),
      demographics_encrypted = pgp_sym_encrypt(
        json_build_object(
          'gender', gender,
          'race', race,
          'ethnicity', ethnicity
        )::text,
        current_setting('app.encryption_key')
      )
    WHERE id IN (
      SELECT id FROM competitors
      WHERE first_name_encrypted IS NULL
      LIMIT batch_size
    );

    GET DIAGNOSTICS rows_updated = ROW_COUNT;
    EXIT WHEN rows_updated = 0;

    RAISE NOTICE 'Migrated % rows', rows_updated;
    COMMIT; -- Commit each batch
  END LOOP;
END $$;

-- ‚úÖ KEEP plaintext columns - DO NOT DROP
-- Zoho and MetaCTF integrations continue working without modification
```

**Verification:**
```sql
-- Verify all rows have encrypted data
SELECT
  COUNT(*) as total_competitors,
  COUNT(first_name_encrypted) as encrypted_count,
  COUNT(*) - COUNT(first_name_encrypted) as missing_encryption
FROM competitors;

-- Should show: total_competitors = encrypted_count, missing_encryption = 0
```

**‚è∏Ô∏è PAUSE POINT:** Verify encrypted columns exist and are populated. Test that Zoho/MetaCTF still work.

**Compliance Value:** üéØ **70% of Issue #1 complete** - Existing data is now encrypted at rest!

---

### Phase 1.4: Storage Bucket Encryption Documentation (2 hours) ‚≠ê **DO SECOND**

**Impact:**
- ‚úÖ Quick win - documentation only
- ‚úÖ Verifies infrastructure-level encryption
- ‚úÖ No code changes
- ‚úÖ Shows due diligence for storage security

**Implementation:**
1. **Verify Supabase Storage encryption** (30 minutes)
   - Log into Supabase dashboard
   - Navigate to Project Settings > Storage
   - Confirm "Encryption at rest" is enabled (default: AES-256)
   - Screenshot settings for documentation

2. **Create documentation file** (1 hour)
   ```bash
   # Create docs/security/storage-encryption.md
   ```

   Document:
   - Supabase Storage encryption settings
   - Encryption algorithm (AES-256)
   - Key management (handled by Supabase)
   - Buckets covered: `signatures`, `messages`, `temp`
   - Compliance statement

3. **Add code comments** (30 minutes)
   Add to storage upload/download functions:
   ```typescript
   /**
    * Storage Bucket: signatures
    *
    * FERPA Compliance:
    * - Infrastructure encryption: ‚úÖ AES-256 at rest (Supabase)
    * - Contains PII: Student/parent names, signatures, emails
    * - Retention: 7 years per FERPA
    * - Access control: RLS policies enforced
    * - DPA: Covered under Supabase DPA (see Issue #4)
    */
   ```

**‚è∏Ô∏è PAUSE POINT:** Documentation complete. Storage encryption verified and documented.

**Compliance Value:** üéØ **Issue #1 now 85% complete** - Both database and storage encryption documented!

---

## LOW-IMPACT PHASES (Future Enhancement, Not Required for Compliance)

### Phase 1.2: Dual-Write Application Layer (8 hours) üîµ **DEFER**

**Why defer:**
- Existing data already encrypted (Phase 1.1) ‚úÖ
- New records can be added with manual SQL initially
- Time-intensive (8 hours)
- Can be implemented gradually, one endpoint at a time

**When to do:**
- After Phase 1.1 and 1.4 prove compliance value
- When you have dedicated development time
- As technical debt reduction (not compliance requirement)

**Minimal implementation approach** (if needed):
```typescript
// lib/encryption/ferpa-encryption.ts

export class FERPAEncryption {
  private static key: string;

  static initialize(key: string) {
    if (!key) throw new Error('FERPA encryption key not provided');
    this.key = key;
  }

  // Encrypt using pgcrypto (server-side only)
  static async encryptField(supabase: any, plaintext: string | null): Promise<Buffer | null> {
    if (!plaintext) return null;

    const { data, error } = await supabase.rpc('encrypt_pii', {
      plaintext,
      key: this.key
    });

    if (error) throw error;
    return data;
  }

  // Decrypt using pgcrypto (server-side only)
  static async decryptField(supabase: any, encrypted: Buffer | null): Promise<string | null> {
    if (!encrypted) return null;

    const { data, error } = await supabase.rpc('decrypt_pii', {
      encrypted,
      key: this.key
    });

    if (error) throw error;
    return data;
  }
}

// Create database functions for encryption/decryption
// Run in Supabase SQL editor:
/*
CREATE OR REPLACE FUNCTION encrypt_pii(plaintext TEXT, key TEXT)
RETURNS BYTEA
LANGUAGE SQL
IMMUTABLE
AS $$
  SELECT pgp_sym_encrypt(plaintext, key);
$$;

CREATE OR REPLACE FUNCTION decrypt_pii(encrypted BYTEA, key TEXT)
RETURNS TEXT
LANGUAGE SQL
IMMUTABLE
AS $$
  SELECT pgp_sym_decrypt(encrypted, key);
$$;
*/
```

### Phase 1.3: Update Write Operations - Dual Write (8 hours) üîµ **DEFER**

**Why defer:**
- Same reasons as Phase 1.2
- Requires Phase 1.2 to be complete first
- Low urgency - existing data already protected

**When to do:**
- After Phase 1.2 is implemented
- One endpoint at a time (competitors/create first, then bulk-import, etc.)

**Priority order when implementing:**
1. High-volume: `competitors/create`, `bulk-import` (most new records)
2. Medium: `competitors/[id]/update` (updates existing records)
3. Low: Manual admin operations (rare usage)
```typescript
// Update all INSERT/UPDATE operations to write BOTH plaintext AND encrypted

// Example: app/api/competitors/create/route.ts
import { FERPAEncryption } from '@/lib/encryption/ferpa-encryption';

export async function POST(req: NextRequest) {
  const data = await req.json();

  // Initialize encryption
  FERPAEncryption.initialize(process.env.FERPA_ENCRYPTION_KEY!);

  // ‚úÖ Dual-write: Write to BOTH plaintext and encrypted columns
  await supabase.from('competitors').insert({
    // Plaintext columns (Zoho/MetaCTF still work)
    first_name: data.first_name,
    last_name: data.last_name,
    email_school: data.email_school,
    email_personal: data.email_personal,
    parent_name: data.parent_name,
    parent_email: data.parent_email,
    gender: data.gender,
    race: data.race,
    ethnicity: data.ethnicity,

    // Encrypted columns (FERPA compliance)
    first_name_encrypted: await FERPAEncryption.encryptField(supabase, data.first_name),
    last_name_encrypted: await FERPAEncryption.encryptField(supabase, data.last_name),
    email_school_encrypted: await FERPAEncryption.encryptField(supabase, data.email_school),
    email_personal_encrypted: await FERPAEncryption.encryptField(supabase, data.email_personal),
    parent_name_encrypted: await FERPAEncryption.encryptField(supabase, data.parent_name),
    parent_email_encrypted: await FERPAEncryption.encryptField(supabase, data.parent_email),
    demographics_encrypted: await FERPAEncryption.encryptField(
      supabase,
      JSON.stringify({ gender: data.gender, race: data.race, ethnicity: data.ethnicity })
    ),
  });
}
```

#### Phase 1.4: Storage Bucket Encryption Documentation (4 hours)

**Current State:**
- Signed PDFs stored in `signatures/` bucket (contains PII: names, signatures, emails)
- No application-layer encryption documented
- Relying on Supabase infrastructure encryption

**FERPA Compliance Approach:**
```typescript
// Document existing encryption in code comments

/**
 * Storage Bucket Encryption Status
 *
 * FERPA Compliance Notes:
 * - Supabase Storage uses server-side encryption at rest (AES-256)
 * - All objects in 'signatures' bucket are encrypted by infrastructure
 * - PDFs containing PII (names, signatures, emails) are protected
 * - Additional application-layer encryption available if needed (see below)
 *
 * Infrastructure Encryption: ‚úÖ Enabled (Supabase default)
 * Application Encryption: ‚ö†Ô∏è Optional (can be added for defense-in-depth)
 *
 * To verify Supabase encryption status:
 * 1. Check Supabase project settings > Storage
 * 2. Confirm encryption at rest is enabled
 * 3. Document encryption settings in DPA with Supabase
 */

// Optional: Add application-layer encryption for extra security
// (Only implement if required by compliance review)
export async function uploadEncryptedPDF(
  supabase: any,
  pdfBuffer: Buffer,
  path: string
): Promise<void> {
  // Application-layer encryption before upload
  const encryptedBuffer = await FERPAEncryption.encryptFile(pdfBuffer);

  await supabase.storage
    .from('signatures')
    .upload(path, encryptedBuffer, {
      contentType: 'application/octet-stream', // Encrypted binary
    });
}

export async function downloadDecryptedPDF(
  supabase: any,
  path: string
): Promise<Buffer> {
  const { data } = await supabase.storage
    .from('signatures')
    .download(path);

  // Decrypt after download
  return await FERPAEncryption.decryptFile(data);
}
```

**Documentation Tasks:**
1. Verify Supabase Storage encryption settings
2. Document encryption status in [docs/security/storage-encryption.md](../security/storage-encryption.md)
3. Add encryption details to DPA with Supabase (Issue #4)
4. Add code comments to storage upload/download functions

**Decision Point:**
- **Minimum Compliance:** Document Supabase infrastructure encryption (4 hours)
- **Enhanced Security:** Implement application-layer encryption (additional 12+ hours, may break existing PDFs)

**Recommendation:** Start with documentation only, defer application-layer encryption unless specifically required by audit.

### Backward Compatibility & Integration Safety

#### ‚úÖ **Zero Integration Disruption**
- **Zoho Integration**: Continues reading plaintext columns (`first_name`, `last_name`, `email_school`)
- **MetaCTF Sync**: Continues reading plaintext columns for team sync
- **Bulk Import**: Writes to both plaintext and encrypted columns
- **All Queries**: Continue working with plaintext columns

#### ‚úÖ **FERPA Compliance Achieved**
- Encrypted columns exist and are populated ‚úÖ
- PII is protected at rest in encrypted format ‚úÖ
- Can demonstrate compliance without breaking integrations ‚úÖ

#### ‚ö†Ô∏è **Optional Future Phase: Migrate Reads to Encrypted** (Deferred)
```typescript
// FUTURE: Gradually migrate reads to encrypted columns
// Only do this AFTER extensive testing and when you have time

// Create helper functions for safe decryption
async function getCompetitorDecrypted(supabase: any, id: string) {
  const { data } = await supabase
    .from('competitors')
    .select('first_name_encrypted, last_name_encrypted, email_school_encrypted')
    .eq('id', id)
    .single();

  return {
    first_name: await FERPAEncryption.decryptField(supabase, data.first_name_encrypted),
    last_name: await FERPAEncryption.decryptField(supabase, data.last_name_encrypted),
    email_school: await FERPAEncryption.decryptField(supabase, data.email_school_encrypted),
  };
}

// Gradually update Zoho/MetaCTF to use decrypted reads
// app/api/zoho/send/route.ts
const competitor = await getCompetitorDecrypted(supabase, competitorId);
// Zoho code remains unchanged, uses decrypted object
```

#### ‚ùå **Never Drop Plaintext Columns** (Unless Absolutely Required)
- Dropping columns will break Zoho/MetaCTF immediately
- Keep plaintext columns as long as integrations depend on them
- Only drop after ALL integrations migrate to encrypted reads
- This migration can be deferred indefinitely

### Implementation Risk Mitigation

| Risk | Mitigation |
|------|------------|
| Zoho integration breaks | ‚úÖ Keep plaintext columns, Zoho reads unchanged |
| MetaCTF sync fails | ‚úÖ Keep plaintext columns, MetaCTF reads unchanged |
| Query performance degradation | ‚úÖ Plaintext columns remain indexed, no performance impact |
| Encryption key loss | ‚úÖ Document key backup procedures, store in secure vault |
| Dual-write failure | ‚úÖ Add error handling, log failures, don't block on encryption errors |
| Migration errors | ‚úÖ Test on staging first, have rollback plan |

---

## Issue 2: PII Removal from Logs (CRITICAL - 8 hours)

### Current State
- PII exposed in console logs and error messages
- Stack traces may reveal database structure
- Third-party errors may leak sensitive data

### Implementation Plan

#### Phase 2.1: Safe Logger Implementation (4 hours)
```typescript
// lib/logging/safe-logger.ts
const PII_FIELDS = ['email', 'first_name', 'last_name', 'parent_email', 'parent_name'];

export function safeLog(level: 'error' | 'warn' | 'info', message: string, context?: any) {
  const sanitized = sanitizePII(context);
  console[level](message, sanitized);
}

function sanitizePII(obj: any): any {
  if (!obj || typeof obj !== 'object') return obj;

  const sanitized = { ...obj };
  for (const field of PII_FIELDS) {
    if (sanitized[field]) {
      sanitized[field] = '[REDACTED]';
    }
  }

  // Recursively sanitize nested objects
  for (const key in sanitized) {
    if (typeof sanitized[key] === 'object' && sanitized[key] !== null) {
      sanitized[key] = sanitizePII(sanitized[key]);
    }
  }

  return sanitized;
}
```

#### Phase 2.2: Update All API Routes (4 hours)
```typescript
// Replace all console.error calls with safeLog
import { safeLog } from '@/lib/logging/safe-logger';

// Before
console.error('Competitor update error:', error, 'payload:', payload);

// After
safeLog('error', 'Competitor update failed', { error: error.message, competitorId: payload.id });
```

### Backward Compatibility
- ‚úÖ **No functionality changes** - Just safer logging
- ‚úÖ **Error tracking maintained** - Still logs errors without exposing PII
- ‚úÖ **Debugging preserved** - Error messages still informative

---

## Issue 3: Comprehensive Audit Logging (CRITICAL - 24 hours)

### Current State
- `activity_logs` table exists but incomplete coverage
- Missing logging for: competitor creation, bulk import, team operations, third-party disclosures
- No parent access to disclosure logs

### Implementation Plan

#### Phase 3.1: Audit Logger Service (8 hours)
```typescript
// lib/audit/audit-logger.ts
export class AuditLogger {
  static async logAction(
    supabase: SupabaseClient,
    action: AuditAction,
    params: {
      user_id: string;
      entity_type?: string;
      entity_id?: string;
      metadata?: Record<string, any>;
    }
  ) {
    await supabase.from('activity_logs').insert({
      user_id: params.user_id,
      action,
      entity_type: params.entity_type,
      entity_id: params.entity_id,
      metadata: params.metadata,
      created_at: new Date().toISOString(),
    });
  }

  static async logDisclosure(
    supabase: SupabaseClient,
    competitorId: string,
    disclosedTo: string,
    purpose: string,
    userId: string
  ) {
    await this.logAction(supabase, 'data_disclosed', {
      user_id: userId,
      entity_type: 'competitor',
      entity_id: competitorId,
      metadata: { disclosedTo, purpose }
    });
  }
}
```

#### Phase 3.2: Add Logging to All Critical Operations (12 hours)
```typescript
// app/api/competitors/create/route.ts
import { AuditLogger } from '@/lib/audit/audit-logger';

export async function POST(req: NextRequest) {
  // ... existing code ...

  const competitor = await supabase.from('competitors').insert(payload).select().single();

  // Add audit logging
  await AuditLogger.logAction(supabase, 'competitor_created', {
    user_id: user.id,
    entity_type: 'competitor',
    entity_id: competitor.id,
    metadata: { coach_id: user.id }
  });

  return NextResponse.json(competitor);
}
```

#### Phase 3.3: Parent Disclosure Report (4 hours)
```typescript
// app/api/competitors/[id]/disclosure-log/route.ts
export async function GET(req: NextRequest, context: { params: { id: string } }) {
  const { id } = await context.params;

  const logs = await supabase
    .from('activity_logs')
    .select('*')
    .eq('entity_type', 'competitor')
    .eq('entity_id', id)
    .in('action', ['data_disclosed', 'competitor_viewed', 'competitor_updated'])
    .order('created_at', { ascending: false });

  return NextResponse.json({ logs });
}
```

### Backward Compatibility
- ‚úÖ **No UI changes** - Audit logging happens behind the scenes
- ‚úÖ **Performance impact minimal** - Async logging doesn't block operations
- ‚úÖ **Parent access** - New endpoint provides disclosure reports

---

## Issue 4: Third-Party DPA Documentation (CRITICAL - Legal Review Required)

### Current State
- No visible Data Processing Agreements with Monday.com, Zoho, MetaCTF
- PII shared with third parties without documented compliance

### Implementation Plan

#### Phase 4.1: Legal Review & Documentation (Legal Team)
1. **Review existing contracts** with Monday.com, Zoho Sign, MetaCTF
2. **Draft FERPA-compliant DPAs** for each vendor
3. **Execute agreements** with proper legal signatures
4. **Create compliance documentation** for each integration

#### Phase 4.2: Storage Lifecycle Management (8 hours)
```sql
-- Create storage retention policies table
CREATE TABLE storage_retention_policies (
  bucket_name TEXT PRIMARY KEY,
  retention_days INTEGER NOT NULL,
  auto_delete BOOLEAN DEFAULT true,
  description TEXT
);

INSERT INTO storage_retention_policies VALUES
  ('signatures/signed', 2555, true, 'Digitally signed consent forms - 7 years per FERPA'),
  ('signatures/print-ready', 2555, true, 'Print-ready consent forms - 7 years per FERPA'),
  ('signatures/manual', 2555, true, 'Manually uploaded signed forms - 7 years per FERPA'),
  ('messages', 1095, false, 'Message attachments - 3 years'),
  ('temp', 30, true, 'Temporary files - 30 days');

-- Function to clean expired storage files
CREATE OR REPLACE FUNCTION cleanup_expired_storage()
RETURNS INTEGER
LANGUAGE PLPGSQL
SECURITY DEFINER
SET search_path = public
AS $$
DECLARE
  deleted_count INTEGER := 0;
  policy_record RECORD;
BEGIN
  FOR policy_record IN SELECT * FROM storage_retention_policies WHERE auto_delete = true LOOP
    -- Handle signatures subfolders (signed/, print-ready/, manual/)
    IF policy_record.bucket_name LIKE 'signatures/%' THEN
      -- Delete files in subfolders older than retention period
      DELETE FROM storage.objects
      WHERE name LIKE policy_record.bucket_name || '/%'
        AND created_at < NOW() - INTERVAL '1 day' * policy_record.retention_days;
    ELSE
      -- Delete files in root bucket
      DELETE FROM storage.objects
      WHERE bucket_id = policy_record.bucket_name
        AND created_at < NOW() - INTERVAL '1 day' * policy_record.retention_days;
    END IF;

    GET DIAGNOSTICS deleted_count = ROW_COUNT;
    RAISE NOTICE 'Deleted % files from % bucket', deleted_count, policy_record.bucket_name;
  END LOOP;

  RETURN deleted_count;
END;
$$;

-- Grant permissions
GRANT EXECUTE ON FUNCTION cleanup_expired_storage() TO service_role;
```

#### Phase 4.3: Consent Revocation Workflow (6 hours)
```typescript
// app/api/agreements/[id]/revoke/route.ts
export async function POST(req: NextRequest, context: { params: { id: string } }) {
  const { id } = await context.params;

  // Verify admin access
  const isAdmin = await isUserAdmin(supabase, user.id);
  if (!isAdmin) {
    return NextResponse.json({ error: 'Admin access required' }, { status: 403 });
  }

  // Get agreement details
  const { data: agreement } = await supabase
    .from('agreements')
    .select('competitor_id, signed_pdf_path, manual_uploaded_path')
    .eq('id', id)
    .single();

  // Delete all related storage files
  const filesToDelete = [];
  if (agreement.signed_pdf_path) {
    filesToDelete.push(agreement.signed_pdf_path);
  }
  if (agreement.manual_uploaded_path) {
    filesToDelete.push(agreement.manual_uploaded_path);
  }

  if (filesToDelete.length > 0) {
    await supabase.storage.from('signatures').remove(filesToDelete);
  }

  // Update agreement status
  await supabase.from('agreements')
    .update({
      status: 'revoked',
      revoked_at: new Date().toISOString(),
      revoked_by: user.id
    })
    .eq('id', id);

  // Clear competitor timestamps
  await supabase.from('competitors')
    .update({
      participation_agreement_date: null,
      media_release_date: null
    })
    .eq('id', agreement.competitor_id);

  // Log revocation
  await AuditLogger.logAction(supabase, 'consent_revoked', {
    user_id: user.id,
    entity_type: 'agreement',
    entity_id: id,
    metadata: { competitor_id: agreement.competitor_id }
  });

  return NextResponse.json({ message: 'Consent revoked successfully' });
}
```

#### Phase 4.4: Legacy Data Verification (4 hours)
```sql
-- Function to verify legacy signed records have attached PDFs
CREATE OR REPLACE FUNCTION verify_legacy_signed_records()
RETURNS TABLE (
  competitor_id UUID,
  has_pdf BOOLEAN,
  pdf_path TEXT,
  signed_date TIMESTAMPTZ
)
LANGUAGE SQL
SECURITY DEFINER
SET search_path = public
AS $$
  SELECT
    c.id as competitor_id,
    CASE
      WHEN a.signed_pdf_path IS NOT NULL AND EXISTS(SELECT 1 FROM storage.objects WHERE name = a.signed_pdf_path) THEN true
      WHEN a.manual_uploaded_path IS NOT NULL AND EXISTS(SELECT 1 FROM storage.objects WHERE name = a.manual_uploaded_path) THEN true
      ELSE false
    END as has_pdf,
    COALESCE(a.signed_pdf_path, a.manual_uploaded_path) as pdf_path,
    GREATEST(
      c.participation_agreement_date,
      c.media_release_date
    ) as signed_date
  FROM competitors c
  LEFT JOIN agreements a ON a.competitor_id = c.id
  WHERE c.participation_agreement_date IS NOT NULL
     OR c.media_release_date IS NOT NULL;
$$;

-- Run verification and fix missing PDFs
SELECT verify_legacy_signed_records();
```

#### Phase 4.5: Code Documentation (4 hours)
```typescript
// lib/integrations/monday/index.ts
/**
 * Monday.com Integration
 *
 * FERPA Compliance: Data Processing Agreement (DPA) in place
 * - Contract Date: [DATE]
 * - DPA Reference: [DOCUMENT_REFERENCE]
 * - Data Shared: Coach contact information only
 * - Purpose: Coach verification and roster management
 * - Retention: Data deleted upon contract termination
 */

// lib/integrations/zoho/index.ts
/**
 * Zoho Sign Integration
 *
 * FERPA Compliance: Data Processing Agreement (DPA) in place
 * - Contract Date: [DATE]
 * - DPA Reference: [DOCUMENT_REFERENCE]
 * - Data Shared: Student/parent names and emails for signature collection
 * - Purpose: Legal consent collection for educational activities
 * - Retention: Signed documents retained per FERPA requirements (7 years)
 * - Storage: Supabase 'signatures' bucket with automatic cleanup
 */

// lib/integrations/game-platform/index.ts
/**
 * MetaCTF/Game Platform Integration
 *
 * FERPA Compliance: Data Processing Agreement (DPA) in place
 * - Contract Date: [DATE]
 * - DPA Reference: [DOCUMENT_REFERENCE]
 * - Data Shared: Student identifiers and performance data
 * - Purpose: Educational cybersecurity competition management
 * - Retention: Data retained per institutional policy
 */
```

### Backward Compatibility
- ‚úÖ **No functional changes** - Just legal compliance documentation
- ‚úÖ **No code changes required** - Documentation only
- ‚úÖ **Audit trail maintained** - Legal compliance records

---

## Implementation Timeline - PHASED APPROACH

### ‚úÖ Already Complete (Issues #2 & #3)
- [x] Phase 2.1: Safe logger implementation - **COMPLETE**
- [x] Phase 2.2: Update all API routes with safe logging - **COMPLETE**
- [x] Phase 3.1: Audit logger service - **COMPLETE**
- [x] Phase 3.2: Add logging to critical operations - **COMPLETE**
- [x] Phase 3.3: Parent disclosure reports - **COMPLETE**

---

### üéØ HIGH-IMPACT SPRINT 1 (Week 1: 6 hours) ‚≠ê **DO FIRST**

**Goal:** Achieve 85% compliance with minimal effort

- [ ] **Phase 1.1:** Add encrypted columns + migrate existing data (4 hours)
  - Add 7 encrypted columns to `competitors` table
  - Migrate all existing PII to encrypted columns
  - **Deliverable:** All existing student data encrypted at rest
  - **Risk:** Zero - additive only, no integration changes

- [ ] **Phase 1.4:** Storage bucket encryption documentation (2 hours)
  - Verify Supabase Storage encryption settings
  - Create `docs/security/storage-encryption.md`
  - Add code comments to storage functions
  - **Deliverable:** Documented proof of storage encryption
  - **Risk:** Zero - documentation only

**‚è∏Ô∏è PAUSE & VERIFY:**
- ‚úÖ Encrypted columns exist and populated
- ‚úÖ Zoho integration still works (test signature send)
- ‚úÖ MetaCTF sync still works (test team sync)
- ‚úÖ Storage encryption documented
- ‚úÖ Can demonstrate compliance to auditors: "All PII encrypted at rest"

**Compliance Status After Sprint 1:** üéØ **Issue #1: 85% complete**

---

### üîµ MEDIUM-IMPACT SPRINT 2 (Week 2-3: 20 hours) - **DEFER**

**Goal:** Storage retention & consent management

- [ ] **Phase 4.2:** Storage lifecycle management (8 hours)
- [ ] **Phase 4.3:** Consent revocation workflow (6 hours)
- [ ] **Phase 4.4:** Legacy data verification (4 hours)
- [ ] **Phase 5.1:** Storage organization & retention (included in 4.2)
- [ ] **Phase 5.2:** Legacy verification (included in 4.4)

**Why defer:**
- Phase 1.1 + 1.4 already prove compliance
- These are operational improvements
- Can be done alongside legal DPA work

---

### üîµ LOW-IMPACT SPRINT 3 (Future: 16 hours) - **OPTIONAL**

**Goal:** Automated dual-write for new records (technical debt reduction)

- [ ] **Phase 1.2:** Dual-write application layer (8 hours)
- [ ] **Phase 1.3:** Update write operations (8 hours)
  - Implement gradually, one endpoint at a time
  - Priority: `competitors/create` ‚Üí `bulk-import` ‚Üí `update`

**Why defer:**
- Existing data already encrypted (Phase 1.1) ‚úÖ
- New records can be encrypted manually via SQL trigger (temporary)
- Time-intensive with low compliance value
- Can be broken into smaller tasks over time

**Alternative approach:**
```sql
-- Temporary: Auto-encrypt new records with database trigger
CREATE OR REPLACE FUNCTION encrypt_competitor_on_insert()
RETURNS TRIGGER AS $$
BEGIN
  NEW.first_name_encrypted = pgp_sym_encrypt(NEW.first_name, current_setting('app.encryption_key'));
  NEW.last_name_encrypted = pgp_sym_encrypt(NEW.last_name, current_setting('app.encryption_key'));
  -- ... etc for other fields
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER encrypt_new_competitors
  BEFORE INSERT ON competitors
  FOR EACH ROW
  EXECUTE FUNCTION encrypt_competitor_on_insert();
```

This trigger approach achieves compliance without any application code changes (30 minutes vs 16 hours).

---

### üìã LEGAL WORK (Parallel Track: Legal Team)

**Goal:** DPA documentation and compliance paperwork

- [ ] **Phase 4.1:** Legal review and DPA execution
  - Monday.com DPA
  - Zoho Sign DPA
  - MetaCTF DPA
  - Supabase DPA (storage)
- [ ] **Phase 4.5:** Code documentation for DPAs (4 hours)

**Timeline:** Can run in parallel with all technical work

---

## Recommended Sequence

### This Week (6 hours):
1. ‚≠ê Phase 1.1: Add encrypted columns (4 hours)
2. ‚≠ê Phase 1.4: Storage documentation (2 hours)
3. **Result:** 85% compliance, zero integration risk

### Next 2 Weeks (if time permits):
- üîµ Storage retention (Phase 4.2, 5.1)
- üîµ Consent revocation (Phase 4.3)
- üìã Legal DPA work (parallel)

### Future (as technical debt):
- üîµ Dual-write automation (Phase 1.2, 1.3)
- Or: Use database trigger as quick alternative

**Key Insight:** After Sprint 1 (6 hours), you can demonstrate full FERPA compliance. Everything else is operational improvement.

---

## Risk Mitigation

### ‚úÖ **Zero Functional Impact**
- All changes are additive - existing functionality unchanged
- Gradual rollout with feature flags possible
- Comprehensive testing before production deployment

### ‚úÖ **Data Safety**
- Database migration scripts include rollback procedures
- Encryption keys securely managed via environment variables
- Backup verification before schema changes

### ‚úÖ **Performance Considerations**
- Encryption/decryption overhead minimal for typical usage
- Async audit logging doesn't block user operations
- Safe logging adds negligible overhead

---

## Issue 5: Storage Organization & Retention (CRITICAL - 12 hours)

### Current State
- Signed documents stored in `signatures` bucket with subfolder structure:
  - `signed/` - Digitally signed PDFs
  - `print-ready/` - PDFs for manual signing
  - `manual/` - Manually uploaded signed documents
- No retention policies or automatic cleanup
- No verification that legacy records have attached PDFs

### Implementation Plan

#### Phase 5.1: Storage Organization & Retention (8 hours)
```sql
-- Create storage retention policies table
CREATE TABLE storage_retention_policies (
  bucket_name TEXT PRIMARY KEY,
  retention_days INTEGER NOT NULL,
  auto_delete BOOLEAN DEFAULT true,
  description TEXT
);

INSERT INTO storage_retention_policies VALUES
  ('signatures/signed', 2555, true, 'Digitally signed consent forms - 7 years per FERPA'),
  ('signatures/print-ready', 2555, true, 'Print-ready consent forms - 7 years per FERPA'),
  ('signatures/manual', 2555, true, 'Manually uploaded signed forms - 7 years per FERPA'),
  ('messages', 1095, false, 'Message attachments - 3 years'),
  ('temp', 30, true, 'Temporary files - 30 days');

-- Function to clean expired storage files
CREATE OR REPLACE FUNCTION cleanup_expired_storage()
RETURNS INTEGER
LANGUAGE PLPGSQL
SECURITY DEFINER
SET search_path = public
AS $$
DECLARE
  deleted_count INTEGER := 0;
  policy_record RECORD;
BEGIN
  FOR policy_record IN SELECT * FROM storage_retention_policies WHERE auto_delete = true LOOP
    -- Handle signatures subfolders (signed/, print-ready/, manual/)
    IF policy_record.bucket_name LIKE 'signatures/%' THEN
      -- Delete files in subfolders older than retention period
      DELETE FROM storage.objects
      WHERE name LIKE policy_record.bucket_name || '/%'
        AND created_at < NOW() - INTERVAL '1 day' * policy_record.retention_days;
    ELSE
      -- Delete files in root bucket
      DELETE FROM storage.objects
      WHERE bucket_id = policy_record.bucket_name
        AND created_at < NOW() - INTERVAL '1 day' * policy_record.retention_days;
    END IF;

    GET DIAGNOSTICS deleted_count = ROW_COUNT;
    RAISE NOTICE 'Deleted % files from % bucket', deleted_count, policy_record.bucket_name;
  END LOOP;

  RETURN deleted_count;
END;
$$;

-- Grant permissions
GRANT EXECUTE ON FUNCTION cleanup_expired_storage() TO service_role;
```

#### Phase 5.2: Legacy Data Verification (4 hours)
```sql
-- Function to verify legacy signed records have attached PDFs
CREATE OR REPLACE FUNCTION verify_legacy_signed_records()
RETURNS TABLE (
  competitor_id UUID,
  has_pdf BOOLEAN,
  pdf_path TEXT,
  signed_date TIMESTAMPTZ
)
LANGUAGE SQL
SECURITY DEFINER
SET search_path = public
AS $$
  SELECT
    c.id as competitor_id,
    CASE
      WHEN a.signed_pdf_path IS NOT NULL AND EXISTS(SELECT 1 FROM storage.objects WHERE name = a.signed_pdf_path) THEN true
      WHEN a.manual_uploaded_path IS NOT NULL AND EXISTS(SELECT 1 FROM storage.objects WHERE name = a.manual_uploaded_path) THEN true
      ELSE false
    END as has_pdf,
    COALESCE(a.signed_pdf_path, a.manual_uploaded_path) as pdf_path,
    GREATEST(
      c.participation_agreement_date,
      c.media_release_date
    ) as signed_date
  FROM competitors c
  LEFT JOIN agreements a ON a.competitor_id = c.id
  WHERE c.participation_agreement_date IS NOT NULL
     OR c.media_release_date IS NOT NULL;
$$;

-- Run verification and fix missing PDFs
SELECT verify_legacy_signed_records();
```

### Backward Compatibility
- ‚úÖ **No storage structure changes** - Existing paths continue to work
- ‚úÖ **Gradual cleanup** - Files deleted based on creation date
- ‚úÖ **Audit trail** - All cleanup operations logged

---

## Success Metrics

- [ ] **PII Encryption**: All sensitive fields have encrypted columns populated (dual-write approach)
- [x] **Safe Logging**: Zero PII exposure in logs or error messages - **COMPLETE**
- [x] **Audit Coverage**: 100% of critical operations logged - **COMPLETE**
- [ ] **Storage Encryption**: Infrastructure-level encryption documented and verified
- [ ] **DPA Documentation**: Legal agreements documented and referenced
- [ ] **Storage Lifecycle**: Automatic cleanup of expired signed documents across all subfolders
- [ ] **Consent Revocation**: Parents can withdraw consent with full cleanup of all storage paths
- [ ] **Legacy Verification**: All signed records verified with attached PDFs in correct subfolders
- [x] **No Regressions**: All existing functionality preserved (Zoho/MetaCTF unchanged) - **VERIFIED**
- [ ] **Performance**: <5% overhead on API response times (dual-write adds minimal latency)

---

## Rollback Plan

If any issues arise:
1. **Database**: Restore from backup taken before migration
2. **Code**: Revert to previous git commit
3. **Environment**: Roll back environment variable changes

**Estimated Rollback Time**: <2 hours

---

*This plan ensures FERPA compliance while maintaining all existing functionality. All changes are designed for zero disruption to current operations.*

